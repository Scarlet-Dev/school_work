---
title: 'Latent Semantic Analysis'
author: "STUDENT NAME"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load the libraries + functions

Load all the libraries or functions that you will use to for the rest of the assignment. It is helpful to define your libraries and functions at the top of a report, so that others can know what they need for the report to compile correctly.

```{r libaries}
##r chunk
library(gutenbergr)
library(stringr)
library(dplyr)
library(tidyr)
```

Load the Python libraries or functions that you will use for that section. 

```{python}
##python chunk
import matplotlib 
matplotlib.use('pdf')

```

## The Data

You will want to use some books from Project Gutenberg to perform a Latent Semantic Analysis. The code to pick the books has been provided for you, so all you would need to do is *change out* the titles. Pick 2 titles from the list below. You can also try other titles not on the list, but they may not work. Check out other book titles at https://www.gutenberg.org/. 

- Book Titles:
    - Crime and Punishment
    - Pride and Prejudice
    - A Christmas Carol
    - The War of the Worlds
    - Twenty Thousand Leagues under the Sea
    - The Iliad
    - The Art of War
    - An Inquiry into the Nature and Causes of the Wealth of Nations
    - Democracy in America â€” Volume 1
    - Dream Psychology: Psychoanalysis for Beginners
    - Talks To Teachers On Psychology; And To Students On Some Of Life\'s Ideals


```{r project_g}
##r chunk
##pick 2 titles from project gutenberg, put in quotes and separate with commas
## DO NOT use the titles used in class, using those will be a 10 point deduction
titles = c()

##read in those books
books = gutenberg_works(title %in% titles) %>%
  gutenberg_download(meta_fields = "title", mirror = "http://mirrors.xmission.com/gutenberg/") %>% 
  mutate(document = row_number())

create_chapters = books %>% 
  group_by(title) %>%
  mutate(chapter = cumsum(str_detect(text, regex("\\bchapter\\b", ignore_case = TRUE)))) %>% 
  ungroup() %>%
  filter(chapter > 0) %>%
  unite(document, title, chapter) 

by_chapter = create_chapters %>% 
  group_by(document) %>% 
  summarise(text=paste(text,collapse=' '))

#by_chapter
```

The `by_chapter` data.frame can be used to create a corpus with `VectorSource` by using the `text` column. 

## Create the Vector Space

Use `tm_map` to clean up the text. 

```{r}
##r chunk 

```

Create a latent semantic analysis model in R. 

```{r}
##r chunk

```

Explore the vector space:
  - Include at least one graphic of the vector space that interests you. 
  - Include at least 2 set of statistics for your model: coherence, cosine, neighbors, etc. 

```{r}
##r chunk

```

Transfer the `by_chapter` to Python and convert it to a list for processing. 

```{python}
##python chunk

```

Process the text using Python. 

```{python}
##python chunk

```

Create the dictionary and term document matrix in Python.

```{python}
##python chunk

```

Find the most likely number of dimensions using the coherence functions from the lecture. 

```{python}
##python chunk

```

Create the LSA model in Python with the optimal number of dimensions from the previous step.

```{python}
##python chunk

```

## Interpretation

Interpret your space - can you see the differences between books/novels? Explain the results from your analysis (at least a 1 paragraph-length explanation). 
  
- ANSWER: 

## Discussion Question

Thinking of your current job or prospective career, propose a research project to address a problem or question in your industry that would use latent semantic analysis. Spend time thinking about this and write roughly a paragraph describing the problem/question and how you would address it with text data and latent semantic analysis.

- ANSWER: 
