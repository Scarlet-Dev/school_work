---
title: 'Part of Speech Tagging Assignment'
author: "Akane Simpson"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
#do not change this
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries / R Setup

- In this section, include the libraries you need for the *R* questions.  

```{r}
##r chunk
# devtools::install_github("bradleyboehmke/harrypotter")
library(harrypotter)

##pick one of the harrypotter books to analyze with your POS text
##https://github.com/bradleyboehmke/harrypotter check out the options
##load it using data(book title)
data("philosophers_stone")

chapter <- philosophers_stone[sample(1:17, 1)]

library(tagger)
library(RDRPOSTagger)
library(dplyr)
```

- In this section, include import functions to load the packages you will use for Python.
- Also transfer your `book_data` from R into Python.  

```{python}
##python chunk
chapter = r.chapter

import spacy
nlp = spacy.load("en_core_web_sm")
import pandas as pd
```

## Tagger Package

- Use the `tagger` package to tag your chosen book and print out the first chapter only (i.e., row 1 of the book you chose). 
- Use something like `(book[1])[[1]][1:10]` to print out the first few tags. 
- Use the universal tag set and plot options to see what the most common parts of speech are for your chapter.
- What are the top two most common parts of speech? 
NOUN and VERB

```{r}
##r chunk
taggerDF <- tidy_pos(tag_pos(chapter))

head(taggerDF)
```


```{r}
tag_pos(chapter) %>% as_universal() %>% plot()
```

```{r}
taggerDF %>%
  group_by(pos) %>% 
  count() %>% 
  arrange(desc(n))
```

## RDR POS Tagger

- Create an English language model that tags for part of speech.
- Tag your first book chapter for part of speech. 
- Use something like `head(...(book[1]))` to print out the first few examples. 

```{r}
##r chunk
create_tagger <- rdr_model(language = "English", annotation = "UniversalPOS")
rdrDF <- rdr_pos(create_tagger, x = chapter)

head(rdrDF)

rdrDF %>%
  group_by(pos) %>% 
  count() %>% 
  arrange(desc(n))
```

```{r}

tagger_summary <-
  taggerDF %>% 
  group_by(pos) %>% 
  count() %>% 
  arrange(desc(n))

rdr_summary <-
  rdrDF %>% 
  group_by(pos) %>% 
  count() %>% 
  arrange(desc(n))

tagger_summary %>%
  full_join(rdr_summary, by = "pos") %>% 
  rename(tagger = n.x,
         rdr = n.y)
```


## spaCy

- Import spacy and the English language module.
- Tag the first chapter of your book using spacy, and print out the results. 
- Use the `pandas` option at the beginning of the lecture to print out only a few rows. 

```{python}
##python chunk
spacy_pos_tagged = [(word, word.tag_, word.pos_) for word in nlp(chapter)]
spacyDF = pd.DataFrame(spacy_pos_tagged, columns = ["token", "specific_pos", "pos"])

spacyDF.head()
```

```{r}
library(reticulate)
spacyDF <- py$spacyDF

head(spacyDF)

spacy_summary <-
  spacyDF %>% 
  group_by(pos) %>% 
  count() %>% 
  arrange(desc(n))

tagger_summary %>%
  full_join(rdr_summary, by = "pos") %>% 
  full_join(spacy_summary, by = "pos") %>% 
  rename(tagger = n.x,
         rdr = n.y, 
         spacy = n)
```


## Training your own tagger

- Create a Default tagger in Python using `nltk`. 
- The default option should be "NN" for nouns.
- You do not have to use the tagger yet, just create it for a combined tagger to use later. (Don't tag! Don't print it out!)

```{python}
import nltk

##python chunk
default_tagger = nltk.DefaultTagger("NN")
```

## Unigram Tagger 

- Create a unigram tagger that is trained on the entire Brown corpus with tagged sentences. 
  - Import the Brown corpus.
  - Split the data into test and train. 
  - Train your unigram tagger on the training sentences.
  - Use the default tagger you created above as the backoff. 
  - Do not use the tagger here, just train it. 

```{python}
##python chunk
from nltk.corpus import brown

brown_tagged_sents = brown.tagged_sents()
brown_tagged_sents[0:5]

##figure out where the split is
size = int(len(brown_tagged_sents) * 0.9)
##break apart the data based on that split
train_sents = brown_tagged_sents[:size]
test_sents = brown_tagged_sents[size:]

unigram_tagger = nltk.UnigramTagger(train_sents, backoff=default_tagger)
```

## Evaluate

- Use the `.evaluate` function on your testing data to determine the accuracy of your tagger. 

```{python}
##python chunk
unigram_tagger.evaluate(test_sents)
# unigram_tagger.accuracy(test_sents)
```

## Apply to Harry Potter

- Use the tagger you created above to apply to the first chapter of your Harry Potter book.
- Hint: save your book as only the first chapter and then transfer to python to help make this shorter. 
- Second hint: be sure to tokenize the data first!
- Use something like `tagger.tag(book)[1:10]` to print out only the first ten tags. 

```{python}
##python chunk
unigram_tagger.tag(nltk.word_tokenize(chapter))[:100]
```

## Compare Results

- Examine the output from the different taggers we used to tag Harry Potter. 
- Are there any immediate differences you can notice when tagging?
- Which tagger seems like the easiest to apply?
- Why might the Brown corpus not be very good at tagging Harry Potter books?


***There was some differing amounts tagging between the POS taggers between both R and Python code. The R code had had more punctuation incorrectly labelled as a noun while the Python code had some issues with word contractions.***


***The RDR tagger was easier to implement than the spaCy and NLTK taggers. Although the RDR tagger required less code to operate it did not have as much customization as the spaCy or NLTK tagger. The spaCy tagger was the second easiest tagger to operate but required some additional parameters than the RDR tagger. The NLTK tagger does require the user to perform some extra code work the NLTK tagger does allow you to create stacked taggers using others as backoffs and can be used to train and evaluate custom tagger using training and testing datasets.***


***The Brown corpus was developed between 1963-1964 so most of the rokens used in this corpus is dated and would not work well for more modern pieces of literature. This would affect how well the Brown corpus is able to accurately tag a modern literature like the Harry Potter book series.***
