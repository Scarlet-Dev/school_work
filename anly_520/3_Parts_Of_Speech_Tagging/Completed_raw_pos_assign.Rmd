---
title: 'Processing Raw Text + Part of Speech Tagging'
author: "Akane Simpson"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Be sure to list the group members at the top! This assignment is the
first component for your class project. Be sure all group members
contribute, *and* each group member must submit the assignment
individually.

You can use either *R* or Python or both for this assignment. You should
use the code and packages we used in class assignments, but these can be
a mix and match of each computing language. You will add the appropriate
type of code chunk for each section depending on the language you pick
for that section.

## Libraries / R Setup

-   In this section, include the libraries you need for the *R*
    questions.

```{r}
##r chunk
library(dplyr)
library(tagger)
library(tokenizers)
library(RDRPOSTagger)
library(reticulate)
```

-   In this section, include import functions to load the packages you
    will use for Python.

```{python}
##python chunk
import re
import os

import ass # I swear to you professor that is what the extension and module are called.
import pandas as pd

import spacy
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
```

## Import Subtitles

Import your subtitle data for the movie or TV show that you selected in
the course proposal.

```{python}

ep1 = ''
with open('/home/arkane/hu_lectures/anly_520/subs/Sub_Sword_Art_Online_01.ass', encoding='utf_8_sig') as f:
    ep1 = ass.parse(f)
```

## Data Cleaning

Clean and format your subtitles so that you end up with a dataframe or
vector that is one line of subtitles for each row (i.e., each spoken
section, not each sentence).

```{python}
## Extract the dialogue from this Event section
dialogue = ep1.sections['Events'][333:]
texts = []

for d in dialogue:
    texts.append(d.text)

texts = texts[:-1]

flatten_texts = " ".join(texts)
```

## Data Normalization

In this section, you should normalize your data with the following
steps:

-   Lower case
-   Remove any weird symbols or other unique formatting

```{python remove and lower}
flatten_texts = re.sub('{\\\i0}', '', flatten_texts)
flatten_texts = re.sub('{\\\i1}', '', flatten_texts)

flatten_texts = re.sub('{\\\q0\\\}', '', flatten_texts)
flatten_texts = re.sub('{\\\q1\\\}', '', flatten_texts)
flatten_texts = re.sub('\{\\\q2\}', '', flatten_texts)

flatten_texts = re.sub('\{would put screw instead of fuck\}', '', flatten_texts)

flatten_texts = re.sub('\{\*\}', '', flatten_texts)
flatten_texts = re.sub('\\\\N', '', flatten_texts)

flatten_texts = flatten_texts.lower()
```

```{python}
with open("/home/arkane/hu_lectures/anly_520/exports/dub_ep1.txt", "w") as f:
	f.write(flatten_texts)
 
```

We will use other data clean up techniques in later sections, as part of
speech tagging requires full words and stopwords.

## Part of Speech Tagging

Select *two* automatic part of speech taggers and tag your subtitles
saving the output in a dataframe. You should use the universal part of
speech tagset to be able to compare the taggers directly.

```{r}
flatten_texts <- py$flatten_texts
tagger <- rdr_model(language = "English", annotation = "UniversalPOS")
rdrDF <- rdr_pos(tagger, x = flatten_texts)

head(rdrDF)
tail(rdrDF)
```

```{python import py speech tagger}
nlp = spacy.load("en_core_web_sm")

spacy_pos_tagged = [(word, word.tag_, word.pos_) for word in nlp(flatten_texts)]
spacyDF = pd.DataFrame(spacy_pos_tagged, columns = ["token", "specific_pos", "pos"])

spacyDF.head()
spacyDF.tail()
```

From these two taggers, calculate a frequency table of the tags (i.e.,
create output that shows the total number of nouns, adjectives, adverbs,
etc.). You can use the `table()` function in *R* or `nltk`'s `FreqDist`
or the counter library. Answer the following questions:

```{r rdr frequency table}
pos_frrq <- rdrDF %>% 
	group_by(pos) %>% 
	count() %>%
	arrange(desc(n))

pos_frrq
```

```{python spacy frequency table}
fd = FreqDist()

for pos in spacyDF["pos"]:
	fd[pos] += 1

fd.tabulate()
```

-   What are the most common part of speech?

> PUNCT, NOUN, PRON and VERB are the most common parts of speech
> identified in this text.

-   Do you appear to have more "action" (verbs), "actors" (nouns), or
    "description" (adjectives and adverbs)? Note here "actor" does not
    mean literal actor on the screen, but the noun that is "acting" out
    the verb.

> We have more actor in this episode.

-   Explain any differences you see in the two part of speech taggers.

> Between the two POS we see that the RDR tagged more nouns and verbs
> than the spaCy POS. This could be due to the different corpus used in
> the two taggers as both used th same text.

-   Which part of speech tagger would you recommend?

> I would recommend the RDR tagger. The spaCy tagger missed a lot of
> possible nouns and verbs in the text that cause a significant df drop
> in tagged nouns and verbs.

Next, calculate a conditional frequency table of the individual words
and their part of speech (i.e., each column should be part of speech,
while the rows are individual words). Answer the following questions:

```{r}
rdrTbls <- xtabs(~ token + pos, data = rdrDF) %>% 
	knitr::kable()
rdrTbls
```

-   What are the most common nouns/actors in your text?

> The most common words in the text are: 'akihiko', 'art', 'body',
> 'boss', 'brain', 'bug', 'button', 'nervegear', 'world', 'game', 'guy',
> 'head', 'face', 'floor', 'kayaba', 'friends', 'line', 'logout',
> 'menu', 'mind', 'people', 'player', 'players', 'reality', 'sao',
> 'skills', 'time', 'town', 'transmitter', 'truth', and 'way'.
>
> There were other words that had a high frequency but may not
> technically count as a noun such as "I'm", 'don't', "can't" "let's",
> "that's", and "what's".

-   What are the most common verbs/actions in your text?

> The most common verbs in the texts are: 'come', 'created', 'feel',
> 'get', 'go', 'going', 'have', 'kidding', 'know', 'let', 'log', 'look',
> 'looks', 'said', 'see', 'think', and 'want'.

-   What are the most common adjectives/adverbs/descriptions in your
    text?

> The most common adjectives in the texts are 'first', 'more', 'online',
> 'real', 'sure', and 'true'. The most common adverbs in the texts are
> 'about', 'at', 'by', 'for', 'from', 'in', 'like', 'of', 'off', 'on',
> 'out', 'up', and 'with'.

-   What words appear to have many parts of speech (i.e., they are not
    only one part of speech but several)?

> In the distribution there were no words that had multiple parts of
> speech. It is possible that the tagger did not find these words but
> the words may be present in the text.

## Interpretation

-   Pretend you've never seen the movie or TV show listed. If you only
    looked at this analysis, what we you be able to determine about the
    show without watching or reading the subtitles?

> From the analysis this dialogue may be in reference to a show based on
> a game called SAO that is hosted online with many players. One of the
> common actor "Akihiko Kayaba" listed here my be related to this game
> in some way as the creator. Additionally the name itself is foreign so
> the setting is outside the western world. The show also mentions a
> possible device that may help host the virtual reality through some
> transmitter using the brain called a nervegear. The conflict here
> might be related to players unable to log out of the game due to some
> sort of bug.
