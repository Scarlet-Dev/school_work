---
title: 'Processing Raw Text Assignment'
author: "Akane Simpson"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
#do not change this
knitr::opts_chunk$set(echo = TRUE)
```

In each step, you will process your data for common text data issues. Be sure to complete each one in *R* and Python separately - creating a clean text version in each language for comparison at the end. Update the saved clean text at each step, do not simply just print it out. 

## Libraries / R Setup

- In this section, include the libraries you need for the *R* questions.  

```{r}
##r chunk
library(rvest)
library(stringr)
```

- In this section, include import functions to load the packages you will use for Python.

```{python}
##python chunk
import requests
from bs4 import BeautifulSoup
import re
```

## Import data to work with

- Use `rvest` to import a webpage and process that text for `html` codes (i.e. take them out)!

```{r}
##r chunk
blog_url <- "https://apnews.com/article/lotteries-iowa-36ef3aefbbc37a15e1c5f172129fa6a6?utm_source=homepage&utm_medium=TopNews&utm_campaign=position_04"

blog_post <- read_html(blog_url)
clean_text <- html_text(blog_post)

clean_text <- substr(clean_text,
       start = str_locate_all(clean_text, "Someone who bought a Powerball")[[1]][2, 1],
       stop = str_locate_all(clean_text, "the U.S. Virgin Islands.")[[1]][1,2])
substr(clean_text, 1, 101)
```

- Use the `requests` package to import the same webpage and use `BeautifulSoup` to clean up the `html` codes.

```{python}
##python chunk 
blog_post = requests.get("https://apnews.com/article/lotteries-iowa-36ef3aefbbc37a15e1c5f172129fa6a6?utm_source=homepage&utm_medium=TopNews&utm_campaign=position_04")
content = blog_post.content

## Beautiful Soup
clean_content = BeautifulSoup(content)
clean_text = clean_content.get_text()

# # clean out the mess
# clean_text = clean_text[re.search("LOS ANGELES \(AP\) — Someone who bought a Powerball", clean_text).start():re.search("the U.S. Virgin Islands.", clean_text).end()]

# cheat to get off bad beginning
clean_text = clean_text[1977:re.search("the U.S. Virgin Islands.", clean_text).end()]
clean_text[0:100]
```

## Lower case

- Lower case the text you created using *R*.

```{r}
##r chunk
clean_text <- tolower(clean_text)
substr(clean_text, 1, 100)
```

- Lower case the text you created using python.

```{python}
##python chunk
clean_text = clean_text.lower()
clean_text[:100]
```

## Removing symbols

- Use the `stringi` package to remove any symbols from your text. 

```{r}
##r chunk
library(stringi)
clean_text <- stri_trans_general(str = clean_text, id = "latin-ascii")
substr(clean_text, 1, 100)
```

- Use the `unicodedata` in python to remove any symbols from your text. 

```{python}
##python chunk
import unicodedata

def remove_accented_chars(text):
  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
  return text

clean_text = remove_accented_chars(clean_text)
clean_text[:100]
```

## Contractions

- Replace all the contractions in your webpage using *R*.

```{r}
##r chunk
library(textclean)

clean_text <- str_replace_all(clean_text,
                        pattern = "’", 
                        replacement = "'")
clean_text <- replace_contraction(clean_text,
                    contraction.key = lexicon::key_contractions, #default
                    ignore.case = T) #default
substr(clean_text, 1, 100)
```

- Replace all the contractions in your webpage using python.

```{python}
##python chunk
import contractions

clean_text = contractions.fix(clean_text)
print(clean_text[:100])
```
  
## Spelling

- Fix any spelling errors with the `hunspell` package in *R* - it's ok to use the first, most probable option, like we did in class. 

```{r}
##r chunk
library(hunspell)
library(tokenizers)

wordlist <- unique(tokenize_words(clean_text))
wordlist <- unique(unlist(tokenize_words(clean_text)))

# Spell check the words
spelling.errors <- hunspell(wordlist) #change wordlist here to clean_text
spelling.sugg <- hunspell_suggest(unique(unlist(spelling.errors)), dict = dictionary("en_US"))

# Pick the first suggestion
spelling.sugg <- unlist(lapply(spelling.sugg, function(x) x[1]))
spelling.dict <- as.data.frame(cbind(spelling.errors = unique(unlist(spelling.errors)),spelling.sugg))
spelling.dict$spelling.pattern <- paste0("\\b", spelling.dict$spelling.errors, "\\b")

#Replace the words
clean_text <- stri_replace_all_regex(str = clean_text,
                       pattern = spelling.dict$spelling.pattern,
                       replacement = spelling.dict$spelling.sugg,
                       vectorize_all = FALSE)
substr(clean_text, 1, 100)
```

- Fix your spelling errors using `textblob` from python. 

```{python}
##python chunk
from textblob import Word
import nltk

clean_tokens = [Word(token).correct() for token in nltk.word_tokenize(clean_text)]
clean_text = " ".join(clean_tokens)
print(clean_text[0:100])

# create a list of unique tokens
# use .correct() to figure out the correct answer
# create a dictionary of key value pairs (word and correct answer) for only the ones that are different
# regex replace using the dictionary
```

## Lemmatization

- Lemmatize your data in *R* using `textstem`. 

```{r}
##r chunk
library(textstem)
clean_text <- lemmatize_strings(clean_text)
substr(clean_text, 1, 100)
```

- Lemmatize your data in python using `spacy`. 

```{python}
##python chunk
import spacy
nlp = spacy.load("en_core_web_sm")

def lemmatize_text(text):
  text = nlp(text)
  text = " ".join([word.lemma_ if word.lemma_ != "-PRON-" else word.text for word in text])
  return text

clean_text = lemmatize_text(clean_text)
print(clean_text[0:100])
```

## Stopwords

- Remove all the stopwords from your *R* clean text. 

```{r}
##r chunk
library(tm)

clean_text <- removeWords(clean_text, stopwords(kind = "SMART"))
substr(clean_text, 1, 100)
```

- Remove all the stop words from your python clean text. 

```{python}
##python chunk
from nltk.corpus import stopwords
set(stopwords.words("english"))
clean_text = [word for word in nltk.word_tokenize(clean_text) if word not in stopwords.words('english')]
clean_text = " ".join(clean_text)
print(clean_text[0:100])
```

## Tokenization 

- Use the `tokenize_words` function to create a set of words for your *R* clean text. 

```{r}
##r chunk
final_words <- tokenize_words(clean_text)
```

- Use `nltk` or `spacy` to tokenize the words from your python clean text. 

```{python}
##python chunk
final_words = nltk.word_tokenize(clean_text)
```

## Check out the results

- Print out the first 100 tokens of your clean text from *R*. 

```{r}
##r chunk
str(final_words)
unlist(final_words)[1:100]
```

- Print out the first 100 tokens of your clean text from python. 

```{python}
##python chunk
print(final_words[:100])
```

Note: here you can print out, summarize, or otherwise view your text in anyway you want. 

- ANSWER THIS: Compare the results from your processing. Write a short paragraph answering the following questions. You will need to write more than a few sentences for credit. 
  - Which text appears to be "cleaner"? 
  - Or are they the same? 
  - What differences can you spot? 
  - Which processing approach appears to be easier? 
  
  ***At first glance the tokens produced by R appeared to be cleaner but the Python tokens seems more inline with the story in the blog post. The R tokens count numbers and some special symbols as tokens which is not necessarily incorrect but it does take away from what tokens can be selected in the first 100 in the token list. Additionally, the R tokens  Likewise, in the Python tokens we see more word tokens being present in the first 100 however there are some mistakes there. The Python tokens does not do well with sentences that end with a period but without a whitespace. The tokenizer sees this as a single word and hence generates a token from it. Again, this is not necessarily incorrect but one will need to review their tokens carefully or perform a better clean to account for sentence errors. Also, the Python lemmatization has some issues with finding the stem of certain words and uses the closest match. We can compare the stem 'verification' from the R tokens versus the supposed Python tokens mirror of it as 'purification'. After reviewing the tokens again I believe the R method is easier to read but Python allows more customization in their methods. Overall, the tokens generated by both methods has their pros and cons but ultimately these can be fixed through better pre-procession on the users part.***
