{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment and Tokenizer functions\n",
    "def sentiment_flag(text: str) -> str:\n",
    "    score = analyzer.polarity_scores(text)\n",
    "    if(score['compound'] >= 0.05):\n",
    "        return 'pos'\n",
    "    elif(score['compound'] <= -0.05):\n",
    "        return 'neg'\n",
    "    else:\n",
    "        return 'neu'\n",
    "\n",
    "def get_polarity(text: str) -> float:\n",
    "    score = analyzer.polarity_scores(text)\n",
    "    return score['compound']\n",
    "\n",
    "def sentence_tokenizer(text: str) -> list[str]:\n",
    "    tokenized = tokenize.word_tokenize(text)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Cleaning\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from markdown import markdown\n",
    "import contractions\n",
    "import unicodedata\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english')) #stopwords\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenize(text: str):\n",
    "    text = markdown(text)\n",
    "    text = re.sub(r'https?://\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = BeautifulSoup(text, \"html.parser\").getText() # HTML decoding\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'0-9', '', text) # remove numbers\n",
    "    \n",
    "    text = text.replace('>', '')\n",
    "    text = text.replace(\"\\\\_\", '')\n",
    "    text = text.replace('&amp', '')\n",
    "    text = text.replace('&gt', '')\n",
    "    text = text.replace('(<a).*(>).*(</a>)', '')\n",
    "    text = text.replace('\\xa0', '')\n",
    "    text = text.replace('<br/>', '')\n",
    "    \n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    text = contractions.fix(text) #contractions # type: ignore\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_sentiment(text: str):\n",
    "    text = markdown(text)\n",
    "    text = re.sub(r'https?://\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = BeautifulSoup(text, \"html.parser\").getText() # HTML decoding\n",
    "    \n",
    "    text = text.lower() # lowercase text\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore') #symbols\n",
    "    text = re.sub(r'0-9', '', text) # remove numbers\n",
    "    text = contractions.fix(text) #contractions # type: ignore\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # type: ignore\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwords from text\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(comment: str) -> int:\n",
    "    comment_list = comment.split()\n",
    "    return len(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import time\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"jdbc:postgresql://localhost:5432/reddit_comments\"\n",
    "\n",
    "properties = {\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"theLegend30$$\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Incel Comments Extraction (01/2020 - 12/2020)\")\\\n",
    "    .config(\"spark.jars\", \"/usr/local/postgres/postgresql-42.6.0.jar\")\\\n",
    "    .config(\"spark.driver.memory\", \"10g\")\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "keysubreddits = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"dbtable\", \"targetted_subreddits\") \\\n",
    "    .option(\"user\", properties[\"user\"]) \\\n",
    "    .option(\"password\", properties[\"password\"])\\\n",
    "    .option(\"driver\", properties[\"driver\"])\\\n",
    "    .load()\n",
    "    \n",
    "keywords = spark.read\\\n",
    "    .format('jdbc')\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"dbtable\", \"ig_array\") \\\n",
    "    .option(\"user\", properties[\"user\"]) \\\n",
    "    .option(\"password\", properties[\"password\"])\\\n",
    "    .option(\"driver\", properties[\"driver\"])\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "keysubredditsResult = keysubreddits.select('*').collect()\n",
    "keywordsResult = keywords.select('*').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "del keywords, keysubreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now will need to separate keywords list and remove '%'\n",
    "keysubredditsList = [word.names for word in keysubredditsResult[:]]\n",
    "keywordsList = [word.replace('%', '') for word in keywordsResult[0].words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "del keysubredditsResult, keywordsResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "del spark, url, properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "incelDF = pd.read_csv('../data/RAW_incelDF-20230615-120758.csv')\n",
    "\n",
    "# Drop the Unnamed and controversality columns\n",
    "incelDF = incelDF.drop('controversiality', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>109437.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.598043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.414665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>203.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               score\n",
       "count  109437.000000\n",
       "mean        0.598043\n",
       "std         4.414665\n",
       "min       -64.000000\n",
       "25%        -1.000000\n",
       "50%         0.000000\n",
       "75%         2.000000\n",
       "max       203.000000"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incelDF.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to Do\n",
    "## EDA\n",
    "In this section we will perform some basic data exploration. We will see\n",
    "\n",
    "- ~~The ratio of subreddit types~~\n",
    "\n",
    "- ~~Size of each subreddits in the search~~\n",
    "\n",
    "- ~~How many subreddits are there~~\n",
    "\n",
    "- ~~How many comments each user has made~~\n",
    "\n",
    "- ~~How many comments are in each subreddit~~\n",
    "\n",
    "- ~~What the average comment length per subreddit~~\n",
    "\n",
    "- ~~What the average word count/length for users~~\n",
    "\n",
    "- ~~What was the average score for each subreddit~~\n",
    "\n",
    "- ~~What were the keywords used in the search~~\n",
    "\n",
    "    \n",
    "\n",
    "## Feature Engineering\n",
    "Additionally, we will need to create new features here such as:\n",
    "\n",
    "- ~~Sentiments for each observation~~\n",
    "\n",
    "- ~~Polarity value for each comment~~\n",
    "\n",
    "- ~~Word tokenization for each comment~~\n",
    "\n",
    "- ~~Two sets of clean text for word tokenization and sentiment flagging~~\n",
    "\n",
    "- ~~Incel Word Frequency per user comment~~"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we imported the incelDF file and dropped the unnamed and controverislality columns from the dataframe. After we pulled data from our local PostgreSQL machine. We used these terms and subreddits to help narrow our search for incel-related langauge. The keywords were supplied by a paper from @gorja and the @incelglossary and were stored in a table. To imprve the search we made use of the PostgreSQL builtin text search engine and added the required symbols to search for these incel terms at the beginning, within or end of words. The subreddits were selected using previously done work on the psyche of incels. From the incel definition and study by [] we made assertions on the type of subreddits we may see incel-related language being used. We tried to avoid using directly incel subreddits or subreddits that act as a counter to incels (i.e IncelTears). in total we have selected 33 subreddits and 152 search words. \n",
    "\n",
    "The search was performed by creating a view of 12 tables, each representing a month of the year where each table containing millions of comments. We then created a Apache Spark session and connected to the PostgreSQL database and then queried the view. The size of this view was of concern so we enabled Apache Arrow to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Subreddits\n",
      "0             dating\n",
      "1            anxiety\n",
      "2   datingoverthirty\n",
      "3    datingoverforty\n",
      "4             advice\n",
      "5              nudes\n",
      "6         truerateme\n",
      "7      dating_advice\n",
      "8         depression\n",
      "9            amiugly\n",
      "10      mentalhealth\n",
      "11            amihot\n",
      "12              self\n",
      "13               sex\n",
      "14     relationships\n",
      "15            tinder\n",
      "16            bumble\n",
      "17         aspergers\n",
      "18              adhd\n",
      "19        ratemycock\n",
      "20      suicidewatch\n",
      "21            lonely\n",
      "22      socialskills\n",
      "23           college\n",
      "24      2meirl4meirl\n",
      "25              rant\n",
      "26             mgtow\n",
      "27   pussypassdenied\n",
      "28      deadbedrooms\n",
      "29              vent\n",
      "30            rateme\n",
      "31          askwomen\n",
      "32      socialskills\n",
      "33            virgin\n"
     ]
    }
   ],
   "source": [
    "subredditsDF = pd.DataFrame(keysubredditsList, columns=[\"Subreddits\"])\n",
    "print(subredditsDF)\n",
    "\n",
    "subredditsDF.to_csv('../data/subreddits.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    List of Search Words\n",
      "0                  alpha\n",
      "1             alpha male\n",
      "2                 ascend\n",
      "3                  awalt\n",
      "4                agecuck\n",
      "..                   ...\n",
      "147                 chad\n",
      "148             chadlite\n",
      "149             gigachad\n",
      "150               tyrone\n",
      "151            normalfag\n",
      "\n",
      "[152 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "keywordsDF = pd.DataFrame(keywordsList, columns=['List of Search Words'])\n",
    "print(keywordsDF)\n",
    "\n",
    "# keywordsDF.to_csv('../data/keywords.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting off with how many subreddit types exist we counted for each row for the type. This yielded two results, either 'public' or 'restricted. A majority of the subreddits (108,959) in this sample were from public communities. Subreddits that are marked as restricted are due to the community and/or its members violation Reddit's sitewide communitiy rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "del keywordsDF, subredditsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public        109378\n",
      "restricted        59\n",
      "Name: subreddit_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "subreddit_type_size = incelDF['subreddit_type'].value_counts()\n",
    "print(subreddit_type_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most of the subreddits are public we dropped the restricted ones and will use this public only dataframe for the rest of our EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "publicIncelDF = incelDF[(incelDF['subreddit_type'] == 'public')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we find the subreddits that have the highest number of comments made. To do this, the grouped the public Incel DF by subreddit and get the size, which acts as a sum of how many uniques subreddits exists in the dataframe. We can see that the top 10 subreedits deal with relationships. The other top 10 subreddits deal with seeking advice, community feedback, mental health and some personal opinions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subreddit\n",
      "relationships       28462\n",
      "sex                 11094\n",
      "Tinder              10363\n",
      "datingoverthirty     7723\n",
      "pussypassdenied      6919\n",
      "dating_advice        6400\n",
      "rant                 5324\n",
      "dating               4736\n",
      "DeadBedrooms         4135\n",
      "Advice               2940\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "subreddit\n",
      "Rateme          485\n",
      "depression      481\n",
      "virgin          419\n",
      "Anxiety         392\n",
      "ADHD            389\n",
      "lonely          285\n",
      "mentalhealth    195\n",
      "amihot          160\n",
      "Nudes            35\n",
      "ratemycock        7\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# First we find the size of each subreddit\n",
    "subreddit_size = publicIncelDF.groupby('subreddit').size().sort_values(ascending=False)\n",
    "print(subreddit_size.head(10))\n",
    "print('\\n')\n",
    "print(subreddit_size.tail(10))\n",
    "\n",
    "subreddit_size.to_csv('../data/subreddit_sizes.csv', header=['size'], index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we find the amount of comments each user has made. The resutls show that the highest number of comments made by a users in this search was 228 comments. The lowest possible number of comments made by a user is one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author\n",
      "myexsparamour       228\n",
      "permanent_staff     145\n",
      "alittlemouth        134\n",
      "Jdamoftruth         120\n",
      "indigo_tortuga      116\n",
      "PSMF_Canuck         113\n",
      "NamelessBard        104\n",
      "AutoModerator        89\n",
      "anus_dei             87\n",
      "facinationstreet     81\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "author\n",
      "WaifuMango              1\n",
      "ExistentialLiberty      1\n",
      "Wait_Wut_Did_E_Say      1\n",
      "Wait__No__What          1\n",
      "WaitingForAFamilyMan    1\n",
      "Waitingforaline         1\n",
      "Wakandalady             1\n",
      "Waketantrum             1\n",
      "Walbaraa                1\n",
      "------why------         1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "most_active_users = publicIncelDF.groupby('author').size().sort_values(ascending=False)\n",
    "print(most_active_users.head(10))\n",
    "print('\\n')\n",
    "print(most_active_users.tail(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table we see that one of the author has the name 'AutoModerator'. This author is actually a bot that the moderators of a community use to inform users, manage community posts and engages with users. We will remove all 'AutoModerator' bots from the dataset. Additionally, we can also drop the subreddit type column since all of the observations are now 'public'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "publicIncelDF = publicIncelDF[publicIncelDF[\"author\"] != 'AutoModerator']\n",
    "publicIncelDF = publicIncelDF.drop('subreddit_type', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we performed some feature engineering. We will\n",
    "\n",
    "- find the length of each comment per user comment\n",
    "- find the word frequency of incel words per comment\n",
    "- find the incel_to_word ratio per user comment length\n",
    "\n",
    "The word frequency feature is of note as we can use this feature to find the incel word frequency in each subreddit. THe incel word ratio is also a helpful metric for measuring how users comments are dominated by or sparsely used incel language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "publicIncelDF['clean_tokenize'] = publicIncelDF['body'].apply(lambda x: clean_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "publicIncelDF['tokenized_text'] = publicIncelDF['clean_tokenize'].apply(lambda a: sentence_tokenizer(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "publicIncelDF['comment_length'] = publicIncelDF['body'].apply(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "publicIncelDF['incel_word_freq'] = publicIncelDF['body'].str.count('|'.join(keywordsList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "publicIncelDF['incel_word_ratio'] = publicIncelDF['incel_word_freq'].div(publicIncelDF['comment_length'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the describe function we see that the average incel ratio is 0.009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>incel_word_freq</th>\n",
       "      <th>incel_word_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>109289.000000</td>\n",
       "      <td>109289.000000</td>\n",
       "      <td>109289.000000</td>\n",
       "      <td>109289.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.597691</td>\n",
       "      <td>70.870078</td>\n",
       "      <td>2.561749</td>\n",
       "      <td>0.054644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.416717</td>\n",
       "      <td>76.148606</td>\n",
       "      <td>2.629011</td>\n",
       "      <td>0.067756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-64.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.063063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>203.000000</td>\n",
       "      <td>1967.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               score  comment_length  incel_word_freq  incel_word_ratio\n",
       "count  109289.000000   109289.000000    109289.000000     109289.000000\n",
       "mean        0.597691       70.870078         2.561749          0.054644\n",
       "std         4.416717       76.148606         2.629011          0.067756\n",
       "min       -64.000000        1.000000         0.000000          0.000000\n",
       "25%        -1.000000       25.000000         1.000000          0.025000\n",
       "50%         0.000000       49.000000         2.000000          0.040000\n",
       "75%         2.000000       90.000000         3.000000          0.063063\n",
       "max       203.000000     1967.000000       101.000000          3.000000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publicIncelDF.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final steps of our feature engineering, we created two bodys of clean text. The first body was used for tokenization. The second clean text was used for sentiment analysis. THis required us to create two separate text cleaning functions. The two text clean function both convert the text into markdown and parsed using BeautifulSoup. Next both functions then lowercased the text, removed numbers and contractions, and were encoded and decoded from ASCII to ITF-8. The first clean function only removes some special characters from the text while the second clean function removes all punctuations and stopwords.\n",
    "\n",
    "The clean_tokenize was then used to create a list of tokens of the sentence and appended to the dataframe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we created sentiment flags per comment by using the 'clean_sentiment' text body and append to our dataframe. Additionally, we created a polarity and sentiment_flag columns. Both use the compund value from the polarity_score return value. We stored the compound value into the polarity column while for the sentiment_flag we created a if-else case to produce either a 'pos', 'neu' or 'neg' flag for each comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "publicIncelDF['clean_sentiment'] = publicIncelDF['body'].apply(lambda y: clean_sentiment(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "publicIncelDF['polarity'] = publicIncelDF['clean_sentiment'].apply(lambda z: get_polarity(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "publicIncelDF['sentiment_flag'] = publicIncelDF['clean_sentiment'].apply(lambda b: sentiment_flag(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_active_users = publicIncelDF.groupby('author').size().sort_values(ascending=False)\n",
    "# print(most_active_users.head(10))\n",
    "# print('\\n')\n",
    "# print(most_active_users.tail(10))\n",
    "\n",
    "# most_active_users.to_csv('../data/most_active_commenters.csv', header=['num_comments'], index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_wordlen_user = publicIncelDF[['author', 'comment_length', 'incel_word_freq']].groupby(['author']).mean()\n",
    "# print(mean_wordlen_user.head(10))\n",
    "# mean_wordlen_user.to_csv('../data/mean_user_comment_len.csv', header=['avg_comment_length', 'avg_incel_word_freq'], index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 109289 entries, 0 to 109436\n",
      "Data columns (total 14 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   id                109289 non-null  object \n",
      " 1   author            109289 non-null  object \n",
      " 2   subreddit         109289 non-null  object \n",
      " 3   body              109289 non-null  object \n",
      " 4   score             109289 non-null  int64  \n",
      " 5   created_on        109289 non-null  object \n",
      " 6   clean_tokenize    109289 non-null  object \n",
      " 7   tokenized_text    109289 non-null  object \n",
      " 8   comment_length    109289 non-null  int64  \n",
      " 9   incel_word_freq   109289 non-null  int64  \n",
      " 10  incel_word_ratio  109289 non-null  float64\n",
      " 11  clean_sentiment   109289 non-null  object \n",
      " 12  polarity          109289 non-null  float64\n",
      " 13  sentiment_flag    109289 non-null  object \n",
      "dtypes: float64(2), int64(3), object(9)\n",
      "memory usage: 12.5+ MB\n"
     ]
    }
   ],
   "source": [
    "publicIncelDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 score  comment_length  incel_word_freq  incel_word_ratio\n",
      "subreddit                                                                \n",
      "2meirl4meirl  0.514432       49.298910         2.187941          0.082903\n",
      "ADHD          0.714653       85.385604         3.020566          0.051423\n",
      "Advice        0.323129       76.546939         2.643537          0.049899\n",
      "Anxiety       0.357143       76.349490         2.790816          0.051585\n",
      "AskWomen      1.082638       67.220243         2.664898          0.058186\n",
      "Bumble        0.270254       51.786578         2.086457          0.063494\n",
      "DeadBedrooms  0.808222       88.696977         3.074728          0.046237\n",
      "MGTOW         0.448579       61.354005         2.423773          0.060063\n",
      "Nudes        -0.628571       32.942857         1.742857          0.131943\n",
      "Rateme        0.410309       41.942268         1.593814          0.071477\n"
     ]
    }
   ],
   "source": [
    "mean_score_subreddit = publicIncelDF[['subreddit', 'score', 'comment_length', 'incel_word_freq', 'incel_word_ratio']].groupby(['subreddit']).mean()\n",
    "print(mean_score_subreddit.head(10))\n",
    "mean_score_subreddit.to_csv('../data/mean_values_subreddit.csv', header=['avg_comment_score', 'avg_comment_length', 'avg_incel_word_freq', 'avg_incel_word_ratio'], index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "publicIncelDF.to_csv('../data/EDA_incelDF'+timestr+'.csv', \n",
    "                     columns=['created_on', 'subreddit', 'score', \n",
    "                              'clean_tokenize', 'tokenized_text',\n",
    "                              'comment_length', 'incel_word_freq',\n",
    "                              'incel_word_ratio', 'clean_sentiment',\n",
    "                              'sentiment_flag', 'polarity'], \n",
    "                     index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
